{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "##############\n",
    "#  Packages  #\n",
    "##############\n",
    "import os\n",
    "import sys \n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from plotly.offline import plot\n",
    "import plotly.io as pio\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "#from umap import UMAP\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "pio.renderers = \"browser\"\n",
    "\n",
    "##################\n",
    "#      Imports   #\n",
    "##################\n",
    "root_path = Path(\"C:/Users/Charles/Desktop/MVA/GDA/Geom_stat/\")\n",
    "sys.path.insert(0, str(root_path))\n",
    "\n",
    "from utils_tda_and_clustering import (\n",
    "    homology_parquet_to_matrix_bootstraps,\n",
    "    make_pca_bootstraps,\n",
    "    get_clusters,\n",
    "    transform_gleason,\n",
    "    meta_clustering,\n",
    "    choose_representative_bootstrap,\n",
    "    get_meta_bootstraps\n",
    ")\n",
    "\n",
    "img_path = root_path.joinpath(\"raw_images\")\n",
    "saving_path = root_path.joinpath(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_name = \"b1\"\n",
    "base_path = str(saving_path.joinpath(f\"{base_name}.parquet\"))\n",
    "df_ident, bootstraps, original = homology_parquet_to_matrix_bootstraps(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"df_ident.pickle\", \"rb\") as f:\n",
    "    df_ident = pickle.load(f)\n",
    "with open(\"bootstraps.pickle\", \"rb\") as f:\n",
    "    bootstraps = pickle.load(f)\n",
    "with open(\"original.pickle\", \"rb\") as f:\n",
    "    original = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_bootstraps, reduced_original, vars_explained = make_pca_bootstraps(bootstraps, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist\n",
    "from gap_statistic import OptimalK\n",
    "\n",
    "def optimalK(reduced_bootstraps,  reduced_original, minClusters=2, maxClusters=10):\n",
    "    \"\"\"\n",
    "    Calculates KMeans optimal K using Gap Statistic \n",
    "    Params:\n",
    "        data: ndarry of shape (n_samples, n_features)\n",
    "        nrefs: number of sample reference datasets to create\n",
    "        maxClusters: Maximum number of clusters to test for\n",
    "    Returns: (gaps, optimalK)\n",
    "    \"\"\"\n",
    "    gaps = np.zeros((len(range(minClusters, maxClusters)),))\n",
    "    resultsdf = {}\n",
    "    for gap_index, k in enumerate(range(minClusters, maxClusters)):# Holder for reference dispersion results\n",
    "        refDisps = np.zeros(len(bootstraps))# For n references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "        for i,b in tqdm(enumerate(reduced_bootstraps)):\n",
    "\n",
    "            clustering_model = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward')\n",
    "            clustering_model.fit(b[\"reduced\"])\n",
    "            clusters_indices = clustering_model.labels_\n",
    "\n",
    "            clusters = [[b[\"reduced\"][j] for j in range(len(clusters_indices)) if clusters_indices[j]==i] for i in range(k)]\n",
    "            \n",
    "            distances = []\n",
    "            for c in clusters:\n",
    "                D_c = np.sum(pdist(c, 'euclidean'))/(2*len(c))\n",
    "                distances.append(D_c)\n",
    "            \n",
    "            bootstrapDisp = np.sum(distances) # The value of W_k for one of our bootstraps\n",
    "            refDisps[i] = bootstrapDisp\n",
    "        \n",
    "        clustering_model = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward')\n",
    "        clustering_model.fit(reduced_original[\"reduced\"])\n",
    "        clusters_indices = clustering_model.labels_\n",
    "        \n",
    "        clusters = [[reduced_original[\"reduced\"][j] for j in range(len(clusters_indices)) if clusters_indices[j]==i] for i in range(k)]\n",
    "        \n",
    "        distances = []\n",
    "        for c in clusters:\n",
    "            D_c = np.sum(pdist(c, 'euclidean'))/(2*len(c))\n",
    "            distances.append(D_c)\n",
    "        \n",
    "        origDisp = np.sum(distances)\n",
    "        \n",
    "        gap = np.mean(np.log(refDisps)) - np.log(origDisp)# Assign this loop's gap statistic to gaps\n",
    "        gaps[gap_index] = gap\n",
    "        \n",
    "        resultsdf[k] = gap\n",
    "    return (gaps.argmax() + minClusters, resultsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimalK(reduced_bootstraps, reduced_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_bootstraps, clustered_original = get_clusters(reduced_bootstraps, reduced_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_tda_and_clustering import transform_gleason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gleason_bootstraps = transform_gleason(bootstraps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "for b in gleason_bootstraps:\n",
    "    clusters = clusters + b[\"clusters\"]\n",
    "gleason_points = np.array([b[\"gleason_coords\"] for b in gleason_bootstraps]).reshape(-1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_clusters_indices = meta_clustering(gleason_points)\n",
    "meta_clusters = [[clusters[k] for k in c] for c in meta_clusters_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_clusters_gleason_coords = [[gleason_points[k] for k in c] for c in meta_clusters_indices]\n",
    "representative_bootstrap = choose_representative_bootstrap(gleason_bootstraps, meta_clusters_gleason_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_bootstraps = get_meta_bootstraps(meta_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_clusters_fused = []\n",
    "for c in meta_clusters:\n",
    "    for cluster in c:\n",
    "        meta_clusters_fused.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:48:48<00:00, 10.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25423914060456126 0.010459507106258568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from utils_tda_and_clustering import get_stability\n",
    "stabilities = []\n",
    "for b in tqdm(meta_bootstraps):\n",
    "    stability = get_stability(b, meta_clusters_fused)\n",
    "    stabilities.append(stability)\n",
    "\n",
    "print(np.mean(stabilities), np.std(stabilities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist\n",
    "from gap_statistic import OptimalK\n",
    "\n",
    "def optimalK_meta(data, n_bootstraps = 100, minClusters=1, maxClusters=10):\n",
    "    \"\"\"\n",
    "    Calculates KMeans optimal K using Gap Statistic \n",
    "    Params:\n",
    "        data: ndarry of shape (n_samples, n_features)\n",
    "        nrefs: number of sample reference datasets to create\n",
    "        maxClusters: Maximum number of clusters to test for\n",
    "    Returns: (gaps, optimalK)\n",
    "    \"\"\"\n",
    "    gaps = np.zeros((len(range(minClusters, maxClusters)),))\n",
    "    gaps_sds = np.zeros_like(gaps)\n",
    "    for gap_index, k in enumerate(range(minClusters, maxClusters)):# Holder for reference dispersion results\n",
    "        refDisps = np.zeros(n_bootstraps)# For n references, generate random sample and perform kmeans getting resulting dispersion of each loop\n",
    "        for i in range(n_bootstraps):\n",
    "\n",
    "            b = np.random.choice(len(data), len(data), replace=True)\n",
    "            b = data[b]\n",
    "\n",
    "            clustering_model = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward')\n",
    "            clustering_model.fit(b)\n",
    "            clusters_indices = clustering_model.labels_\n",
    "\n",
    "            clusters = [[b[j] for j in range(len(clusters_indices)) if clusters_indices[j]==i] for i in range(k)]\n",
    "            \n",
    "            distances = []\n",
    "            for c in clusters:\n",
    "                D_c = np.sum(pdist(c, 'euclidean'))/(2*len(c))\n",
    "                distances.append(D_c)\n",
    "            \n",
    "            bootstrapDisp = np.sum(distances) # The value of W_k for one of our bootstraps\n",
    "            refDisps[i] = np.log(bootstrapDisp)\n",
    "        \n",
    "        clustering_model = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward')\n",
    "        clustering_model.fit(data)\n",
    "        clusters_indices = clustering_model.labels_\n",
    "        \n",
    "        clusters = [[data[j] for j in range(len(clusters_indices)) if clusters_indices[j]==i] for i in range(k)]\n",
    "        \n",
    "        distances = []\n",
    "        for c in clusters:\n",
    "            D_c = np.sum(pdist(c, 'euclidean'))/(2*len(c))\n",
    "            distances.append(D_c)\n",
    "        \n",
    "        origDisp = np.sum(distances)\n",
    "        \n",
    "        gap = np.mean(refDisps) - np.log(origDisp)# Assign this loop's gap statistic to gaps\n",
    "        gaps[gap_index] = gap\n",
    "\n",
    "        gaps_sds[gap_index] = np.std(refDisps)\n",
    "        \n",
    "    return (gaps, gaps_sds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
